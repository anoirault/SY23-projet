{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation inspiré de l'article originale\n",
    "## Import de librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle utilisé par le critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, inChannel:int, outChannel:int):\n",
    "        super().__init__()\n",
    "        self.inLayer = nn.Sequential(\n",
    "            nn.Linear(inChannel, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.linLayer = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.outLayer = nn.Sequential(\n",
    "            nn.Linear(32, outChannel)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.inLayer(x.float())\n",
    "        x = self.linLayer(x)\n",
    "\n",
    "        return self.outLayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle utilisé pour l'acteur, génère les paramètres de la distribution à utiliser pour choisir l'action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDistribParam(nn.Module):\n",
    "    def __init__(self, inChannel):\n",
    "        super().__init__()\n",
    "        self.inLayer = nn.Sequential(\n",
    "            nn.Linear(inChannel, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.linLayer1 = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.muLayer = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.sigmaLayer = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.inLayer(x.float())\n",
    "        x = self.linLayer1(x)\n",
    "        return self.muLayer(x),torch.log(1+torch.exp(self.sigmaLayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition de l'acteur qui décidera de l'action à prendre en fonction de la *policy* aprise ainsi que le critic qui évalura la qualité de la décision prise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_lr = lambda epoch: 0.5\n",
    "class Policy():\n",
    "    def __init__(self, dim_state:int, lr=5e-3):\n",
    "        self.lr = lr\n",
    "        self.reg = NormalDistribParam(inChannel=dim_state) \n",
    "        self.optim = torch.optim.Adam(self.reg.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiplicativeLR(self.optim, lambda_lr)\n",
    "\n",
    "class Critic():\n",
    "    def __init__(self, dim_state:int, lr=5e-3):\n",
    "      self.lr = lr\n",
    "      self.reg = Regressor(inChannel = dim_state, outChannel = 1)\n",
    "      self.optim = torch.optim.Adam(self.reg.parameters(), lr=self.lr)\n",
    "      self.scheduler = torch.optim.lr_scheduler.MultiplicativeLR(self.optim, lambda_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des fonctions utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choisi une action et l'évalue, utilisé pour générer les données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy, critic, state):\n",
    "    mean, std = policy.reg(torch.tensor(state))\n",
    "\n",
    "    distrib = Normal(mean+1e-7, std+1e-7)\n",
    "\n",
    "    action = distrib.sample()\n",
    "    p = distrib.log_prob(action)\n",
    "    v = critic.reg(torch.tensor(state))\n",
    "\n",
    "    return action.detach(), p.detach(), v.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisé lors de la phase d'apprentissage, retourne les données à optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, critic, state, action):\n",
    "    mean, std = policy.reg(torch.tensor(state))\n",
    "\n",
    "    distrib = Normal(mean+1e-7, std+1e-7)\n",
    "\n",
    "    p = distrib.log_prob(action)\n",
    "    v = critic.reg(torch.tensor(state))\n",
    "    entropy = distrib.entropy()\n",
    "\n",
    "    return p, v, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "évalue la policy aprise, retourne les résultats moyennés sur n jeux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, critic, n):\n",
    "    n_iter = 0\n",
    "    r = 0\n",
    "    for _ in range(n):\n",
    "        term, trunc = False, False\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while (not term or trunc):\n",
    "            action, _, _ = choose_action(policy, critic, state)\n",
    "\n",
    "            state, reward, term, trunc = env.step(action)\n",
    "\n",
    "            n_iter += 1\n",
    "            r += reward\n",
    "    return n_iter/n, r/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation des données d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50\n",
    "N = 100\n",
    "T = 3\n",
    "epochs = 50\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "gamma = .95\n",
    "epsilon = .1\n",
    "percent = .50\n",
    "\n",
    "c1, c2 = 0.01,0.0001\n",
    "\n",
    "env_name = \"InvertedDoublePendulum-v4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "policy = Policy(dim_state=env.observation_space.shape[0])\n",
    "critic = Critic(dim_state=env.observation_space.shape[0])\n",
    "\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A exécuter pour charger les poids des models pré-entrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_pol = torch.load(\"model-ppo/from-paper/\"+env_name+\"_pol.pt\")\n",
    "policy.reg.load_state_dict(ckp_pol[\"model\"])\n",
    "\n",
    "ckp_critic = torch.load(\"model-ppo/from-paper/\"+env_name+\"_critic.pt\")\n",
    "critic.reg.load_state_dict(ckp_critic[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du *custom Dataset* de pytorch\n",
    "\n",
    "L'utilisation d'un Dataset et Dataloader permet l'apprentissage en batch de manière très simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes pour séparer les données de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_IDX_LOW = 0\n",
    "STATE_IDX_UP = env.env.observation_space.shape[0]\n",
    "ACTION_IDX = STATE_IDX_UP\n",
    "ADV_IDX = 1 + STATE_IDX_UP\n",
    "P_IDX = 2 + STATE_IDX_UP\n",
    "V_IDX = 3 + STATE_IDX_UP\n",
    "\n",
    "print(STATE_IDX_LOW, STATE_IDX_UP, ACTION_IDX, ADV_IDX, P_IDX, V_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, data:list, partition:float):\n",
    "      self.data = torch.cat(data[:int(len(data)*partition)], dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        old_state = self.data[idx][STATE_IDX_LOW : STATE_IDX_UP+1]\n",
    "        old_action = self.data[idx][ACTION_IDX]\n",
    "        old_adv = self.data[idx][ADV_IDX]\n",
    "        old_p = self.data[idx][P_IDX]\n",
    "        old_V = self.data[idx][V_IDX]\n",
    "\n",
    "        return old_state, old_action, old_adv, old_p, old_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_r = []\n",
    "graph_iter = []\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    old_buff = []\n",
    "    print(int(max(T, n_iter-2)))\n",
    "    for n_actor in range(N):\n",
    "\n",
    "        t_t = []\n",
    "        t_V = []\n",
    "        t_adv = []\n",
    "        t_reward = []\n",
    "\n",
    "        t_buff = []\n",
    "\n",
    "        term = True\n",
    "        trunc = True\n",
    "        t = 0\n",
    "\n",
    "        for timesteps in range(int(max(T, n_iter-1))):\n",
    "            t += 1\n",
    "            if term or trunc:\n",
    "                t = 0\n",
    "                state, _ = env.reset()\n",
    "\n",
    "            action, p, v = choose_action(policy, critic, state)\n",
    "            state, reward, term, trunc = env.step(action)\n",
    "\n",
    "            t_reward.append(reward)\n",
    "\n",
    "            t_t.append(t)\n",
    "            t_V.append(sum([t_reward[j]*gamma**j for j in range(timesteps-t+1, timesteps+1)]))\n",
    "            t_adv.append(t_V[-1]-v)\n",
    "\n",
    "            state = tuple(state)\n",
    "            \n",
    "            t_buff.append([*state, action, t_V[-1]-v, p, t_V[-1]])\n",
    "\n",
    "\n",
    "        old_buff.append(torch.tensor(t_buff, dtype=torch.float32))\n",
    "\n",
    "    shuffle(old_buff)\n",
    "\n",
    "    train_data = SegmentDataset(old_buff, percent)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for old_state, old_action, old_adv, old_p, old_V in train_dataloader:\n",
    "\n",
    "            newP, newV, entropy = evaluate(policy, critic, state, action)\n",
    "\n",
    "            ratio = torch.exp(newP - old_p)\n",
    "\n",
    "            loss1 = ratio * old_adv\n",
    "            loss2 = torch.clamp(ratio, 1-epsilon, 1+epsilon)*old_adv\n",
    "\n",
    "            l_clip = torch.min(loss1, loss2)\n",
    "            l_vf = mse(newV, torch.tensor(old_V))\n",
    "\n",
    "            loss = -(l_clip - c1*l_vf + c2*entropy)\n",
    "            loss = loss.mean()\n",
    "            policy.optim.zero_grad()\n",
    "            critic.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            policy.optim.step()\n",
    "            critic.optim.step()\n",
    "            \n",
    "            avg_loss += loss.detach()\n",
    "        print(f'Epoch [{epoch}/{epochs}] Mean Loss {avg_loss/(int(N*T*percent))}')\n",
    "\n",
    "    n_iter, r = test_policy(env, policy, critic, 20)\n",
    "    graph_iter.append(n_iter)\n",
    "    graph_r.append(r)\n",
    "    print(f\"Iter[{iteration}/{iterations}]  n_iter mean : {n_iter} reward mean : {r}\")\n",
    "    if iteration != 0 and iteration%10:\n",
    "        policy.scheduler.step()\n",
    "        critic.scheduler.step()\n",
    "\n",
    "torch.save({\"model\":policy.reg.state_dict()}, \"policy.pt\")\n",
    "torch.save({\"model\":critic.reg.state_dict()}, \"critic.pt\")\n",
    "plt.figure()\n",
    "plt.plot(graph_iter)\n",
    "plt.plot(graph_r)\n",
    "plt.grid()\n",
    "plt.legend([\"iter\", \"reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter, r = test_policy(env, policy, critic, 20)\n",
    "print(n_iter, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Comparaison avec le tutoriel de Pythorch](https://pytorch.org/rl/tutorials/coding_ppo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" if not torch.has_cuda else \"cuda:0\"\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 1\n",
    "frames_per_batch = 1000 // frame_skip\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 10_000 // frame_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device, frame_skip=frame_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(\n",
    "            in_keys=[\"observation\"],\n",
    "        ),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"done_spec:\", env.done_spec)\n",
    "print(\"action_spec:\", env.action_spec)\n",
    "print(\"state_spec:\", env.state_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.minimum,\n",
    "        \"max\": env.action_spec.space.maximum,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor=policy_module,\n",
    "    critic=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    value_target_key=advantage_module.value_target_key,\n",
    "    critic_coef=1.0,\n",
    "    gamma=0.99,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames * frame_skip)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        with torch.no_grad():\n",
    "            advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optim step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel() * frame_skip)\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our env horizon).\n",
    "        # The ``rollout`` method of the env can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
