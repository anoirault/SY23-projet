{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de l'algorithme REINFORCE\n",
    "\n",
    "L'implémentation proposé est inspiré par le tutoriel de [Gymnasium](https://gymnasium.farama.org/tutorials/training_agents/reinforce_invpend_gym_v26/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import de librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle de regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDistribParam(nn.Module):\n",
    "    def __init__(self, inChannel):\n",
    "        super().__init__()\n",
    "        self.inLayer = nn.Sequential(\n",
    "            nn.Linear(inChannel, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.linLayer1 = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.muLayer = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.sigmaLayer = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.inLayer(x.float())\n",
    "        x = self.linLayer1(x)\n",
    "        return self.muLayer(x),torch.log(1+torch.exp(self.sigmaLayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition de la *policy* à optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, inChannel, lr) -> None:\n",
    "        self.reg = NormalDistribParam(inChannel=inChannel)\n",
    "        self.opti = torch.optim.AdamW(self.reg.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opti, gamma=.9)\n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        mean, std = self.reg(state)\n",
    "        \n",
    "        distrib = Normal(mean+1e-7, std+1e-7) # ajout d'une constante pour la stabilité \n",
    "        \n",
    "        action = distrib.sample()\n",
    "        p = distrib.log_prob(action)\n",
    "        \n",
    "        return action, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(liste:list)->list:\n",
    "    return sum(liste)/len(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, n):\n",
    "    n_iter = 0\n",
    "    r = 0\n",
    "    for _ in range(n):\n",
    "        term, trunc = False, False\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        while (not term or trunc):\n",
    "            action, _, _ = policy.choose_action(state)\n",
    "\n",
    "            state, reward, term, trunc = env.step(action)\n",
    "\n",
    "            n_iter += 1\n",
    "            r += reward\n",
    "    return n_iter/n, r/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpisode = 29000 \n",
    "frequenceUpdate = 250 \n",
    "\n",
    "gamma = .9\n",
    "\n",
    "env_name = \"InvertedDoublePendulum-v4\"\n",
    "\n",
    "l_ps = []\n",
    "l_rewards = []\n",
    "avg_reward = []\n",
    "ttlRewardEpisode = []\n",
    "lenEpisode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "policy = Policy(inChannel=env.observation_space[0], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1, nEpisode+1):\n",
    "    state, _ = env.reset()\n",
    "    running = True\n",
    "    l_reward = []\n",
    "    l_p = []\n",
    "    t = 0\n",
    "    \n",
    "    while running:\n",
    "        \n",
    "        action, p = policy.choose_action(state=state)\n",
    "        state, r, terminated, truncated, _ = env.step(action=action)\n",
    "        l_p.append(p)\n",
    "        running = not (terminated or truncated)\n",
    "        if running:\n",
    "            l_reward.append(r)\n",
    "            avg_reward.append(r)\n",
    "        else:\n",
    "            l_reward.append(-1)\n",
    "            avg_reward.append(-1)\n",
    "            \n",
    "    l_ps.append(l_p)\n",
    "    l_rewards.append(l_reward)\n",
    "    ttlRewardEpisode.append(sum(l_reward))\n",
    "    lenEpisode.append(len(l_p))\n",
    "    \n",
    "    if episode % frequenceUpdate == 0:\n",
    "        loss = 0\n",
    "        policy.opti.zero_grad()\n",
    "        \n",
    "        print(\"AVG nStep : \", mean([len(l_ps[n]) for n in range(len(l_ps))]))\n",
    "        for n in range(len(l_ps)):\n",
    "            for t in range(len(l_ps[n])):\n",
    "                loss += -1*l_ps[n][t]*torch.sum(torch.tensor([l_rewards[n][t]*gamma**(t_prime-t) for t_prime in range(t, len(l_ps[n]))]))\n",
    "\n",
    "        loss.backward()\n",
    "        policy.optimizer.step()\n",
    "        \n",
    "        if(episode%(frequenceUpdate*5)==0):\n",
    "            policy.scheduler.step()\n",
    "        print(f\"Episode : {episode}/{nEpisode} Average Reward : {mean(avg_rewards)}\")\n",
    "        l_ps = []\n",
    "        l_rewards = []\n",
    "        avg_rewards = []\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ttlRewardEpisode)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Total Reward per episode\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lenEpisode)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Length of an episode\")\n",
    "plt.title(\"Length of each episode episode\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
